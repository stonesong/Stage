\section{Analyse des études passés}
\label{sec:analyse}

Nous procédons dans cette partie à l’analyse d’études passés sur la prédiction de bugs qui ont utilisés les métriques de procédés pour la prédiction de bugs. Nous évaluons si les valeurs des métriques de procédés pourraient être biaisés en regardant la façon dont ils ont recueillis leurs données.\\
Enfin nous donnons quelques lignes de conduite à suivre pour aider les chercheurs et développeurs à éviter l’impact que le renommage d’entités pourrait avoir sur les métriques de procédés.  

\subsection{Analysis of Past Studies}

Premièrement, il est important de remarquer que, comme nous l'avons montré dans la \secref{results}, les périodes contenant un taux de fichiers renommés élevés sont rares. Ainsi, la majeure partie des études passés ne devraient pas être affectés par ce phénomène. De plus, même dans le cas d'analyses sur des périodes contenant un taux élevé de fichiers renommés, les résultats de ces analyses pourrait également être améliorées, car les métriques de procédés auraient probablement été sous-estimées. Toutefois, plusieurs études passées peuvent être affectées par le renommage, comme nous le signalerons dans la suite de cette section. Quantifier de tels effets sur les études passés est hors de porté de notre sujet, mais nous fournissons tout de même certaines ligne de conduite à respecter pour les études future dans \secref{guidelines}.



In this analysis of past studies, we include only the 26 studies referenced in~\cite{radjenovic_software_2013} that use the CC, NoD or NoC metrics. However several other studies referenced in~\cite{radjenovic_software_2013} use slightly different change metrics and could also be impacted on renaming.

Dans notre étude des études passées, nous regardons 26 articles référencés dans ~\cite{radjenovic_software_2013} qui pouvaient traiter les trois métriques de procédés, CC, NoD ou NoC. Cependant certaines des autre études référencées dans cet article utilisent d'autre métriques de procédés et pourrait donc aussi être affectées par le renommage.\\

$15$ de ces études analyses des projets industriels, ~\cite{arisholm_systematic_2010,graves_predicting_2000,khoshgoftaar_using_2000,layman_iterative_2008,munson_code_1998,nagappan_use_2005,nagappan_influence_2008,nagappan_using_2007,nagappan_using_2006,nagappan_change_2010,nikora_building_2006,ostrand_programmer-based_2010,weyuker_too_2008,weyuker_using_2007,yuan_application_2000}. Aucune de ces études ne parle de renommage, mais le manque d'information récoltées sur les VCS utilisés et sur le projet en lui-même ne nous permet pas de savoir si le renommage pouvait avoir un impact sur ces projets. Néanmoins, l'article de Kim et al ~\cite{kim_field_2012} explique que les développeurs dans son étude effectuent des opérations de refactoring, dont du renommage, sans utiliser les outils du VCS appropriés. Ainsi, ces études pourraient être impactées par le renommage en fonction des outils utilisés et des habitudes de développement.\\

$11$ études analysent des logiciels open-source \cite{dambros_relationship_2009,bacchelli_are_2010,caglayan_merits_2009,dambros_evaluating_2012,dambros_evaluating_2012,dambros_extensive_2010,illes-seifert_exploring_2010,li_finding_2005,matsumoto_analysis_2010,moser_analysis_2008,moser_comparative_2008,schroter_if_2006}. Les VCS utilisés dans ces études sont CVS ou Subversion. CVS ne gère pas le renommage et Subversion uniquement de manière manuelle ce qui est dangereux comme expliqué dans l'article ~\cite{lavoie_inferring_2012,steidl_incremental_2014}. Seulement deux de ces études ~\cite{moser_analysis_2008,moser_comparative_2008} parlent de renommage dans leur set de données ou dans les ''Threats to validiy''. Pour réduire le risque d'erreur dans leurs expérimentations, ces deux études ont supprimé systématiquement tous les fichiers ajoutés ou supprimés durant les périodes analysées. C'est un bon moyen d'éviter de calculer des métriques de procédés biaisés, mais cela implique aussi de supprimer inutilement du jeu de données un nombre significatif de fichiers.\\

\subsection{Guidelines}
\label{sec:guidelines}

According to the results of our two experiments, we deduce several simple guidelines to compute change metrics. Firstly, we recommend to avoid computing such metrics during initial periods of projects at all costs. Indeed, these periods usually contain a significant amount of renaming. As we have seen, both major and minor periods can contain a significant amount of renaming, although major release seems more prone to renaming. In any case, we recommend to systematically use a renaming detection algorithm, to avoid picking up the wrong period. Git provides a dedicated algorithm that seems to have a good precision, but an unknown recall. Therefore using projects managed by Git seems the easier way to lower the threat of renaming. More advanced renaming detection algorithms are also described in the literature:~\cite{antoniol_automatic_2004,lavoie_inferring_2012,steidl_incremental_2014}. They have been empirically validated so they might perform better than Git's algorithm, and are the only choices if the chosen corpus contains project that are not managed by Git. Finally, for change metrics computed at finer level of granularity than files, we recommend the use of origin analysis algorithms such as~\cite{wu_aura:_2010}. These algorithms usually work at the granularity of the functions. Finally, as artifact renaming can be a significant threat, we recommend to systematically indicate how it was dealt with in future studies.
