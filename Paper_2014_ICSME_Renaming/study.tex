\section{Analysis of Past Studies}
\label{sec:study}

In this section, we proceed to an analysis of the past studies that used change metrics to predict defects. We evaluate if the values of changes metrics could be biased by analyzing how they collected their data. Finally, we give some guidelines to help researchers and practitioners to avoid the impact that artifact renaming has on change metrics.

\subsection{Analysis of Past Studies}

Firstly, it is important to remark that as we have shown in \secref{results}, periods containing a high amount of renaming are rare. Therefore, most of the past studies should be not affected by this phenomenon. Additionally, even in the case of using periods having a significant amount of renaming, the results of such studies could also be improved, because change metrics would probably have been underestimated. Nevertheless, several past studies can be impacted by renaming, as we will point out in the remainder of this section. Quantifying such effect on these past studies is out of the scope of this paper, but we provide guidelines for future studies in \secref{guidelines}.

In this analysis of past studies, we include only the 26 studies referenced in~\cite{radjenovic_software_2013} that use the CC, NoD or NoC metrics. However several other studies referenced in~\cite{radjenovic_software_2013} use slightly different change metrics and could also be impacted on renaming.

$15$ past studies use industrial software projects~\cite{arisholm_systematic_2010,graves_predicting_2000,khoshgoftaar_using_2000,layman_iterative_2008,munson_code_1998,nagappan_use_2005,nagappan_influence_2008,nagappan_using_2007,nagappan_using_2006,nagappan_change_2010,nikora_building_2006,ostrand_programmer-based_2010,weyuker_too_2008,weyuker_using_2007,yuan_application_2000}. All these studies did not list artifact renaming anywhere in the data collection or threats to validity sections. Unfortunately, the lack of information about VCSs and software projects used in these studies forbid us to evaluate if artifact renaming is a possible bias for their result. However, the article of Kim et al.~\cite{kim_field_2012} points out the fact that industrial developers can also perform refactoring (including renaming) without using the dedicated tool. Therefore these studies might be impacted depending on the tools and habits of developers involved in the industrial projects.

$11$ studies use open-source software projects\cite{dambros_relationship_2009,bacchelli_are_2010,caglayan_merits_2009,dambros_evaluating_2012,dambros_evaluating_2012,dambros_extensive_2010,illes-seifert_exploring_2010,li_finding_2005,matsumoto_analysis_2010,moser_analysis_2008,moser_comparative_2008,schroter_if_2006}. The VCSs used by the projects included in these studies are either CVS or Subversion. CVS do not handle renaming, and Subversion handles renaming manually which is dangerous as explained in~\cite{lavoie_inferring_2012,steidl_incremental_2014}. However, only $2$ of these studies~\cite{moser_analysis_2008,moser_comparative_2008} listed artifact renaming in their data collection or threat to validity sections. To mitigate the risk of artifact renaming, these 2 studies deleted from their corpus files that have been added or removed during the analyzed periods. This is an effective way of avoiding computing skewed change metric values. However, it can remove unnecessarily a significant amount of files from the corpus, which in turn might bias the study. In conclusion, all these studies might also be impacted by artifact renaming.

\subsection{Guidelines}
\label{sec:guidelines}

According to the results of our two experiments, we deduce several simple guidelines to compute change metrics. Firstly, we recommend to avoid computing such metrics during initial periods of projects at all costs. Indeed, these periods usually contain a significant amount of renaming. As we have seen, both major and minor periods can contain a significant amount of renaming, although major release seems more prone to renaming. In any case, we recommend to systematically use a renaming detection algorithm, to avoid picking up the wrong period. Git provides a dedicated algorithm that seems to have a good precision, but an unknown recall. Therefore using projects managed by Git seems the easier way to lower the threat of renaming. More advanced renaming detection algorithms are also described in the literature:~\cite{antoniol_automatic_2004,lavoie_inferring_2012,steidl_incremental_2014}. They have been empirically validated so they might perform better than Git's algorithm, and are the only choices if the chosen corpus contains project that are not managed by Git. Finally, for change metrics computed at finer level of granularity than files, we recommend the use of origin analysis algorithms such as~\cite{wu_aura:_2010}. These algorithms usually work at the granularity of the functions. Finally, as artifact renaming can be a significant threat, we recommend to systematically indicate how it was dealt with in future studies.
